import streamlit as st
import cv2
import torch
import numpy as np
import os
from PIL import Image
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
import time
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase, RTCConfiguration
import tempfile
import io

# --- C·∫§U H√åNH TRANG WEB ---
st.set_page_config(
    page_title="So s√°nh Face Recognition Models",
    page_icon="ü§ñ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- T·∫¢I MODEL V√Ä D·ªÆ LI·ªÜU (CACHE ƒê·ªÇ TƒÇNG T·ªêC) ---
@st.cache_resource
def load_all_models():
    """T·∫£i t·∫•t c·∫£ c√°c model AI m·ªôt l·∫ßn duy nh·∫•t."""
    from facenet_pytorch import MTCNN, InceptionResnetV1
    from insightface.app import FaceAnalysis

    print("ƒêang t·∫£i models... (Ch·ªâ ch·∫°y m·ªôt l·∫ßn)")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    mtcnn_model = MTCNN(keep_all=True, device=device)
    resnetv1_model = InceptionResnetV1(pretrained='vggface2').eval().to(device)
    arcface_model = FaceAnalysis(name='buffalo_l', providers=['CUDAExecutionProvider' if device.type == 'cuda' else 'CPUExecutionProvider'])
    arcface_model.prepare(ctx_id=0 if device.type == 'cuda' else -1)
    
    print("T·∫£i models th√†nh c√¥ng.")
    return mtcnn_model, resnetv1_model, arcface_model, device

@st.cache_data
def load_known_face_data_from_file(model_name):
    """T·∫£i d·ªØ li·ªáu khu√¥n m·∫∑t ƒë√£ l∆∞u t·ª´ c√°c file .npy ban ƒë·∫ßu."""
    print(f"ƒêang t·∫£i d·ªØ li·ªáu g·ªëc cho {model_name}...")
    emb_file = f'known_{model_name}_embeddings.npy'
    name_file = f'known_{model_name}_names.npy'
    if os.path.exists(emb_file) and os.path.exists(name_file):
        embeddings = list(np.load(emb_file, allow_pickle=True))
        names = list(np.load(name_file, allow_pickle=True))
        return embeddings, names
    return [], []

def recognize_face(embedding, known_embeddings, known_names, threshold=0.6):
    if len(known_embeddings) == 0:
        return "Unknown", 0.0
    similarities = cosine_similarity([embedding], known_embeddings)[0]
    max_idx = np.argmax(similarities)
    max_sim = similarities[max_idx]
    return (known_names[max_idx], max_sim) if max_sim >= threshold else ("Unknown", max_sim)

# T·∫£i t√†i nguy√™n
mtcnn, resnetv1, arcface_app, device = load_all_models()

# --- S·ª¨ D·ª§NG SESSION STATE ƒê·ªÇ L∆ØU TR·ªÆ D·ªÆ LI·ªÜU ---
if 'initialized' not in st.session_state:
    st.session_state.known_resnetv1_embeddings, st.session_state.known_resnetv1_names = load_known_face_data_from_file("facenet")
    st.session_state.known_arcface_embeddings, st.session_state.known_arcface_names = load_known_face_data_from_file("arcface")
    st.session_state.initialized = True
    st.sidebar.success("T·∫•t c·∫£ c√°c model v√† d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c t·∫£i.")

# --- L·ªöP X·ª¨ L√ù VIDEO TH·ªúI GIAN TH·ª∞C CHO STREAMLIT-WEBRTC ---
class VideoTransformer(VideoTransformerBase):
    def __init__(self):
        self.threshold = 0.6

    def recv(self, frame):
        img = frame.to_ndarray(format="bgr24")
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        
        # X·ª≠ l√Ω v·ªõi ResnetV1
        boxes, _ = mtcnn.detect(img_rgb)
        if boxes is not None:
            for box in boxes:
                face_tensor = mtcnn.extract(img_rgb, [box], save_path=None).to(device)
                embedding = resnetv1(face_tensor).detach().cpu().numpy()[0]
                name, sim = recognize_face(embedding, st.session_state.known_resnetv1_embeddings, st.session_state.known_resnetv1_names, self.threshold)
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(img, f'R: {name} ({sim:.2f})', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)

        # X·ª≠ l√Ω v·ªõi ArcFace
        faces = arcface_app.get(img)
        if len(faces) > 0:
            for face in faces:
                arc_embedding = face.embedding
                name, sim = recognize_face(arc_embedding, st.session_state.known_arcface_embeddings, st.session_state.known_arcface_names, self.threshold)
                x1, y1, x2, y2 = face.bbox.astype(int)
                cv2.rectangle(img, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(img, f'A: {name} ({sim:.2f})', (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)

        return img

# --- GIAO DI·ªÜN THANH B√äN (SIDEBAR) ---
with st.sidebar:
    st.title("Computer Vision Project")
    st.header("So s√°nh FaceNet v√† ArcFace")
    st.write("**Sinh vi√™n th·ª±c hi·ªán:** Th√°i")
    st.info("Ch·ªçn c√°c tab b√™n d∆∞·ªõi ƒë·ªÉ xem chi ti·∫øt.")
    
    st.subheader("S·ªë ng∆∞·ªùi trong CSDL:")
    st.write(f"**{len(np.unique(st.session_state.known_resnetv1_names))}** ng∆∞·ªùi")

# --- N·ªòI DUNG CH√çNH ---
st.title("Nh·∫≠n di·ªán khu√¥n m·∫∑t: So s√°nh ResNetV1 & ArcFace")

tab1, tab2, tab3, tab4 = st.tabs([
    "So s√°nh T·ªïng quan", 
    "Demo Tr·ª±c ti·∫øp",
    "Qu·∫£n l√Ω D·ªØ li·ªáu",
    "Gi·ªõi thi·ªáu D·ª± √°n"
])

# --- TAB 1: SO S√ÅNH T·ªîNG QUAN ---
with tab1:
    st.header("So s√°nh Hi·ªáu su·∫•t v√† Ki·∫øn tr√∫c")
    st.subheader("1. B·∫£ng so s√°nh Ki·∫øn tr√∫c")
    try:
        df_arch = pd.read_csv("architecture_comparison.csv")
        st.dataframe(df_arch.style.format(precision=2), use_container_width=True)
    except FileNotFoundError:
        st.error("Kh√¥ng t√¨m th·∫•y file `architecture_comparison.csv`.")
    st.subheader("2. B·∫£ng so s√°nh tr√™n Benchmark")
    try:
        df_benchmark = pd.read_csv("benchmark_comparison.csv")
        st.dataframe(df_benchmark.style.format(precision=2), use_container_width=True)
    except FileNotFoundError:
        st.error("Kh√¥ng t√¨m th·∫•y file `benchmark_comparison.csv`.")
    st.divider()
    st.subheader("3. Bi·ªÉu ƒë·ªì So s√°nh T·ªïng h·ª£p")
    if os.path.exists("comparison_charts_full.png"):
        st.image("comparison_charts_full.png", caption="Bi·ªÉu ƒë·ªì so s√°nh hi·ªáu su·∫•t, ki·∫øn tr√∫c v√† benchmark.")
    else:
        st.warning("Kh√¥ng t√¨m th·∫•y file 'comparison_charts_full.png'.")

# --- TAB 2: DEMO TR·ª∞C TI·∫æP ---
with tab2:
    st.header("Th·ª≠ nghi·ªám Nh·∫≠n di·ªán")
    
    demo_tab1, demo_tab2, demo_tab3 = st.tabs(["üì∏ Webcam Th·ªùi gian th·ª±c", "üñºÔ∏è Ph√¢n t√≠ch ·∫¢nh", "üé¨ Ph√¢n t√≠ch Video"])

    # --- Demo qua Webcam Th·ªùi gian th·ª±c ---
    with demo_tab1:
        st.info("Nh·∫•n n√∫t 'START' ƒë·ªÉ b·∫≠t webcam v√† xem k·∫øt qu·∫£ nh·∫≠n di·ªán theo th·ªùi gian th·ª±c. Nh·∫•n 'STOP' ƒë·ªÉ d·ª´ng.")
        webrtc_streamer(
            key="realtime-recognition",
            video_transformer_factory=VideoTransformer,
            media_stream_constraints={"video": True, "audio": False},
            async_processing=True,
        )

    # --- Demo qua ·∫¢nh t·∫£i l√™n ---
    with demo_tab2:
        st.write("T·∫£i l√™n m·ªôt b·ª©c ·∫£nh c√≥ ch·ª©a khu√¥n m·∫∑t ƒë·ªÉ xem k·∫øt qu·∫£ nh·∫≠n di·ªán t·ª´ c·∫£ hai m√¥ h√¨nh.")
        uploaded_file = st.file_uploader("Ch·ªçn m·ªôt file ·∫£nh", type=["jpg", "jpeg", "png"], key="img_uploader")
        if uploaded_file is not None:
            image = Image.open(uploaded_file).convert('RGB')
            frame_rgb = np.array(image)
            frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)
            st.image(image, caption='·∫¢nh g·ªëc', use_column_width=True)
            with st.spinner('ƒêang ph√¢n t√≠ch...'):
                # (Logic x·ª≠ l√Ω ·∫£nh gi·ªØ nguy√™n)
                boxes, _ = mtcnn.detect(frame_rgb)
                if boxes is not None:
                    for box in boxes:
                        face_tensor = mtcnn.extract(frame_rgb, [box], save_path=None).to(device)
                        embedding = resnetv1(face_tensor).detach().cpu().numpy()[0]
                        name, sim = recognize_face(embedding, st.session_state.known_resnetv1_embeddings, st.session_state.known_resnetv1_names)
                        x1, y1, x2, y2 = map(int, box)
                        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(frame_bgr, f'R: {name} ({sim:.2f})', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                faces = arcface_app.get(frame_bgr)
                if len(faces) > 0:
                    for face in faces:
                        arc_embedding = face.embedding
                        name, sim = recognize_face(arc_embedding, st.session_state.known_arcface_embeddings, st.session_state.known_arcface_names)
                        x1, y1, x2, y2 = face.bbox.astype(int)
                        cv2.rectangle(frame_bgr, (x1, y1), (x2, y2), (255, 0, 0), 2)
                        cv2.putText(frame_bgr, f'A: {name} ({sim:.2f})', (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
            st.image(frame_bgr, caption='K·∫øt qu·∫£ Nh·∫≠n di·ªán', channels="BGR", use_column_width=True)

    # --- Demo qua Video t·∫£i l√™n ---
    with demo_tab3:
        st.write("T·∫£i l√™n m·ªôt file video ƒë·ªÉ x·ª≠ l√Ω v√† nh·∫≠n di·ªán khu√¥n m·∫∑t trong ƒë√≥.")
        uploaded_video = st.file_uploader("Ch·ªçn m·ªôt file video", type=["mp4", "mov", "avi", "mkv"], key="video_uploader")

        if uploaded_video is not None:
            tfile = tempfile.NamedTemporaryFile(delete=False)
            tfile.write(uploaded_video.read())
            
            cap = cv2.VideoCapture(tfile.name)
            
            frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = int(cap.get(cv2.CAP_PROP_FPS))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

            output_path = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False).name
            out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))
            
            st.success(f"ƒê√£ t·∫£i l√™n video. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {total_frames} frames...")
            progress_bar = st.progress(0, text="ƒêang x·ª≠ l√Ω...")
            
            frame_count = 0
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                
                # √Åp d·ª•ng logic nh·∫≠n di·ªán cho t·ª´ng frame
                img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                boxes, _ = mtcnn.detect(img_rgb)
                if boxes is not None:
                    for box in boxes:
                        face_tensor = mtcnn.extract(img_rgb, [box], save_path=None).to(device)
                        embedding = resnetv1(face_tensor).detach().cpu().numpy()[0]
                        name, sim = recognize_face(embedding, st.session_state.known_resnetv1_embeddings, st.session_state.known_resnetv1_names)
                        x1, y1, x2, y2 = map(int, box)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                        cv2.putText(frame, f'R: {name} ({sim:.2f})', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                faces = arcface_app.get(frame)
                if len(faces) > 0:
                    for face in faces:
                        arc_embedding = face.embedding
                        name, sim = recognize_face(arc_embedding, st.session_state.known_arcface_embeddings, st.session_state.known_arcface_names)
                        x1, y1, x2, y2 = face.bbox.astype(int)
                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)
                        cv2.putText(frame, f'A: {name} ({sim:.2f})', (x1, y2 + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
                
                out.write(frame)
                frame_count += 1
                progress_bar.progress(frame_count / total_frames, text=f"ƒêang x·ª≠ l√Ω frame {frame_count}/{total_frames}")

            cap.release()
            out.release()
            os.unlink(tfile.name)
            
            progress_bar.empty()
            st.success("X·ª≠ l√Ω video ho√†n t·∫•t!")
            st.video(output_path)
            os.unlink(output_path)


# --- TAB 3: QU·∫¢N L√ù D·ªÆ LI·ªÜU ---
with tab3:
    st.header("Qu·∫£n l√Ω C∆° s·ªü d·ªØ li·ªáu Khu√¥n m·∫∑t")
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Th√™m ng∆∞·ªùi m·ªõi")
        add_name = st.text_input("Nh·∫≠p t√™n:", key="add_name_tab3")
        add_uploaded_img = st.file_uploader("T·∫£i ·∫£nh khu√¥n m·∫∑t", type=["jpg", "jpeg", "png"], key="add_img_tab3")
        if st.button("‚ûï Th√™m"):
            if add_uploaded_img and add_name.strip():
                # (Logic th√™m ng∆∞·ªùi gi·ªØ nguy√™n)
                with st.spinner(f"ƒêang th√™m {add_name}..."):
                    img = Image.open(add_uploaded_img).convert("RGB")
                    img_array = np.array(img)
                    boxes, _ = mtcnn.detect(img_array)
                    if boxes is not None:
                        face_tensor = mtcnn.extract(img_array, [boxes[0]], save_path=None).to(device)
                        resnetv1_emb = resnetv1(face_tensor).detach().cpu().numpy()[0]
                        st.session_state.known_resnetv1_embeddings.append(resnetv1_emb)
                        st.session_state.known_resnetv1_names.append(add_name.strip())
                        faces = arcface_app.get(cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR))
                        if len(faces) > 0:
                            arc_emb = faces[0].embedding
                            st.session_state.known_arcface_embeddings.append(arc_emb)
                            st.session_state.known_arcface_names.append(add_name.strip())
                            np.save('known_facenet_embeddings.npy', np.array(st.session_state.known_resnetv1_embeddings))
                            np.save('known_facenet_names.npy', np.array(st.session_state.known_resnetv1_names))
                            np.save('known_arcface_embeddings.npy', np.array(st.session_state.known_arcface_embeddings))
                            np.save('known_arcface_names.npy', np.array(st.session_state.known_arcface_names))
                            st.success(f"ƒê√£ th√™m '{add_name.strip()}'.")
                            st.rerun()
                        else:
                            st.warning("ArcFace kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t.")
                            st.session_state.known_resnetv1_embeddings.pop()
                            st.session_state.known_resnetv1_names.pop()
                    else:
                        st.warning("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t.")
            else:
                st.warning("Vui l√≤ng t·∫£i ·∫£nh v√† nh·∫≠p t√™n!")

    with col2:
        st.subheader("Xo√° ng∆∞·ªùi ƒë√£ ƒëƒÉng k√Ω")
        if len(st.session_state.known_resnetv1_names) > 0:
            unique_names = sorted(list(np.unique(st.session_state.known_resnetv1_names)))
            to_delete = st.selectbox("Ch·ªçn t√™n ƒë·ªÉ xo√°", unique_names, key="delete_name", index=None, placeholder="Ch·ªçn m·ªôt t√™n...")
            if st.button("‚ùå Xo√°") and to_delete:
                # (Logic x√≥a ng∆∞·ªùi gi·ªØ nguy√™n)
                new_resnet_emb, new_resnet_names = [], []
                new_arcface_emb, new_arcface_names = [], []
                for i, name in enumerate(st.session_state.known_resnetv1_names):
                    if name != to_delete:
                        new_resnet_emb.append(st.session_state.known_resnetv1_embeddings[i])
                        new_resnet_names.append(name)
                        new_arcface_emb.append(st.session_state.known_arcface_embeddings[i])
                        new_arcface_names.append(name)
                st.session_state.known_resnetv1_embeddings = new_resnet_emb
                st.session_state.known_resnetv1_names = new_resnet_names
                st.session_state.known_arcface_embeddings = new_arcface_emb
                st.session_state.known_arcface_names = new_arcface_names
                np.save('known_facenet_embeddings.npy', np.array(new_resnet_emb))
                np.save('known_facenet_names.npy', np.array(new_resnet_names))
                np.save('known_arcface_embeddings.npy', np.array(new_arcface_emb))
                np.save('known_arcface_names.npy', np.array(new_arcface_names))
                st.success(f"ƒê√£ xo√° t·∫•t c·∫£ ·∫£nh c·ªßa '{to_delete}'.")
                st.rerun()
        else:
            st.write("Ch∆∞a c√≥ ai trong c∆° s·ªü d·ªØ li·ªáu.")

# --- TAB 4: GI·ªöI THI·ªÜU D·ª∞ √ÅN ---
with tab4:
    st.header("M·ª•c ti√™u v√† Ph∆∞∆°ng ph√°p")
    # (Gi·ªØ nguy√™n n·ªôi dung tab 3)
    st.markdown("""
    D·ª± √°n n√†y ƒë∆∞·ª£c th·ª±c hi·ªán trong khu√¥n kh·ªï m√¥n h·ªçc Th·ªã gi√°c m√°y t√≠nh, nh·∫±m m·ª•c ƒë√≠ch so s√°nh hai m√¥ h√¨nh nh·∫≠n d·∫°ng khu√¥n m·∫∑t ti√™n ti·∫øn: **FaceNet (s·ª≠ d·ª•ng ki·∫øn tr√∫c InceptionResnetV1)** v√† **ArcFace (s·ª≠ d·ª•ng m√¥ h√¨nh buffalo_l)**.

    ### Ph∆∞∆°ng ph√°p so s√°nh
    Ch√∫ng t√¥i ƒë√°nh gi√° hai m√¥ h√¨nh d·ª±a tr√™n c√°c ti√™u ch√≠ sau:
    1.  **Hi·ªáu su·∫•t tr√™n c√°c b·ªô d·ªØ li·ªáu benchmark chu·∫©n.**
    2.  **ƒê·∫∑c ƒëi·ªÉm ki·∫øn tr√∫c (S·ªë tham s·ªë, FLOPs).**
    3.  **Hi·ªáu su·∫•t trong th·ªùi gian th·ª±c (FPS, ƒê·ªô ·ªïn ƒë·ªãnh).**
    4.  **Th·ª≠ nghi·ªám ƒë·ªãnh t√≠nh qua ·∫£nh ng∆∞·ªùi d√πng t·∫£i l√™n v√† webcam.**
    
    To√†n b·ªô ·ª©ng d·ª•ng demo n√†y ƒë∆∞·ª£c x√¢y d·ª±ng b·∫±ng **Streamlit**.
    """)
